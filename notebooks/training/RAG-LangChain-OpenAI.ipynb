{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Retrieval-Augmented generation (RAG)\n",
    "\n",
    "RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs.\n",
    "\n",
    "<img src=\"../figures/RAG-process.png\" >\n",
    "\n",
    "Introducing `SwaraBot`, an innovative chatbot that answers questions about Swaraj based on his personal document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<This has been intentionally removed for security reasons. All you must do is replace it with your own OpenAI API key.>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompt\n",
    "\n",
    "A set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template=\"You are a helpful assistant that answers questions about the person based on their personal documents.\\n    Use the following context to answer the question. If you don't know the answer, just say you don't know.\\n    Don't make things up.    \\n\\n    Context: {context}\\n    Question: {question}\\n    Answer:\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    You are a helpful assistant that answers questions about the person based on their personal documents.\n",
    "    Use the following context to answer the question. If you don't know the answer, just say you don't know.\n",
    "    Don't make things up.    \n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "    template = prompt_template\n",
    ")\n",
    "\n",
    "PROMPT\n",
    "#using str.format \n",
    "#The placeholder is defined using curly brackets: {} {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a helpful assistant that answers questions about the person based on their personal documents.\\n    Use the following context to answer the question. If you don't know the answer, just say you don't know.\\n    Don't make things up.    \\n\\n    Context: This is a sample context about the person. The person has a background in computer science and has worked on various projects related to artificial intelligence.\\n    Question: What is the person's background?\\n    Answer:\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT.format(\n",
    "    context = \"This is a sample context about the person. The person has a background in computer science and has worked on various projects related to artificial intelligence.\",\n",
    "    question = \"What is the person's background?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : [How to improve prompting (Zero-shot, Few-shot, Chain-of-Thought, etc.](https://github.com/chaklam-silpasuwanchai/Natural-Language-Processing/blob/main/Code/05%20-%20RAG/advance/cot-tot-prompting.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval\n",
    "\n",
    "1. `Document loaders` : Load documents from many different sources (HTML, PDF, code). \n",
    "2. `Document transformers` : One of the essential steps in document retrieval is breaking down a large document into smaller, relevant chunks to enhance the retrieval process.\n",
    "3. `Text embedding models` : Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of text that are similar.\n",
    "4. `Vector stores`: there has emerged a need for databases to support efficient storage and searching of these embeddings.\n",
    "5. `Retrievers` : Once the data is in the database, you still need to retrieve it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Document Loaders \n",
    "Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "[PDF Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)\n",
    "\n",
    "[Download Document](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "nlp_docs = '../pdfs/Swaraj.pdf'\n",
    "\n",
    "loader = PyMuPDFLoader(nlp_docs)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='contributed to building mass update tools that led to a notable decrease in ticket volumes. \\nHis impact extended to bug ﬁxes, outbound interface conﬁgurations, and transaction \\nmonitoring, making him an indispensable asset.  \\nElevated to Senior Software Engineer (Jan 2024 - July 2024, Remote), he expanded his \\nscope by implementing timezone-based PDF exports, developing a centralized API retry \\nmechanism, and enhancing auto-reprocessing for supplier-side transactions. His \\nscheduler solution for detecting and removing stuck transactions further improved \\nefficiency.  \\nCurrently, as an AI Engineer at AI Brain Lab (Jan 2025 - Present, On-Site, Bangkok \\nMetropolitan Region, Thailand), he has successfully dockerized the frontend of the ESG \\napplication, implemented CI/CD pipelines for automated deployment, and optimized \\npackage management service segregation, reducing redundancy in the application. His \\ncontributions have accelerated deployment processes and improved infrastructure \\nefficiency. \\n \\nProfessional Experience \\nSwaraj has extensive expertise in multi-cloud architecture, container orchestration, and \\nCI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \\nzero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \\nReact, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \\nbuilding scalable web applications.  \\nHis expertise in UI/UX and API development allows him to optimize and streamline system \\ninteractions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \\nRedis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \\noptimization in NoSQL and deployed Redis caching solutions to improve query \\nperformance and data retrieval speeds.  \\nSwaraj has hands-on experience with PyTorch, TensorFlow, and Hugging Face models, \\ntraining BERT, etc. He also built a Dockerized MLﬂow Server (GCP), integrated with a Flask \\nAPI for ML model versioning.  \\nHe has a deep understanding of networking and security, successfully deploying a personal \\nVPN on Raspberry Pi that allowed a user in China to securely browse the internet via a \\nThailand server. His reverse SSH tunneling system further strengthened remote access \\nsecurity. \\n \\n', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Document Transformers\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "doc = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \\nInstitute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \\nBachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \\nRanchi (2017-2021).  \\n \\nWork  Proﬁle \\nSwaraj started his career at GEP Worldwide, where he played an instrumental role in \\noptimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \\ntenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \\nSenior Software Engineer.  \\nAs a Product Development Intern (May 2020 - July 2020, Remote), he developed a React', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Text Embedding Models\n",
    "Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
    "\n",
    "*Note* Instructor Model : [Huggingface](gingface.co/hkunlp/instructor-base) | [Paper](https://arxiv.org/abs/2212.09741)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axiom/anaconda3/envs/NLP6/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Vector Stores\n",
    "\n",
    "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate vectorstore\n",
    "vector_path = '../models/vector-store-openai'\n",
    "if not os.path.exists(vector_path):\n",
    "    os.makedirs(vector_path)\n",
    "    print('create path done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save vector locally\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents = doc,\n",
    "    embedding = embedding_model\n",
    ")\n",
    "\n",
    "db_file_name = 'nlp_stanford'\n",
    "\n",
    "vectordb.save_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    index_name = 'nlp' #default index\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 retrievers\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling vector from local\n",
    "vector_path = '../models/vector-store-openai'\n",
    "db_file_name = 'nlp_stanford'\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.load_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    embeddings = embedding_model,\n",
    "    index_name = 'nlp' #default index \n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ready to use\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Personal Aspirations \\nUltimately, Swaraj’s ambition is to become a businessman, leveraging his technical \\nexpertise, problem-solving mindset, and strategic vision to create impactful tech solutions.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 3, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \\nInstitute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \\nBachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \\nRanchi (2017-2021).  \\n \\nWork  Proﬁle \\nSwaraj started his career at GEP Worldwide, where he played an instrumental role in \\noptimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \\ntenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \\nSenior Software Engineer.  \\nAs a Product Development Intern (May 2020 - July 2020, Remote), he developed a React', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Introduction \\nSwaraj Bhanja is a highly skilled Cloud Engineer, AI Enthusiast, and Technologist with \\nexpertise spanning cloud computing, DevOps, full-stack development, machine learning, \\nand AI infrastructure optimization. His journey has been deﬁned by deep technical \\nexpertise, an insatiable curiosity for problem-solving, and a relentless drive for efficiency.  \\n \\nPersonal Information \\nBorn on October 21, 1997, in Jamshedpur, Jharkhand, India, he hails from Kochi, Kerala, \\nand is currently based in Bangkok, Thailand, where he works as an AI Engineer at AI Brain \\nLab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React \\nNative-based RFID application, created a secure QR code generator with custom \\nencryption, and built a QR code scanner for secure data retrieval. His innovation laid the \\ngroundwork for secure mobile-based authentication systems.  \\nIn his Technology Internship (Jan 2021 - June 2021, Remote), he worked extensively with \\nSQL databases, creating generic SQL scripts for data correction requests and building a UI-\\nbased tool for monitoring and manually processing failed transactions. His work \\nstreamlined internal processes, reducing the turnaround time for resolving transactional \\nfailures.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is the person's background?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React \\nNative-based RFID application, created a secure QR code generator with custom \\nencryption, and built a QR code scanner for secure data retrieval. His innovation laid the \\ngroundwork for secure mobile-based authentication systems.  \\nIn his Technology Internship (Jan 2021 - June 2021, Remote), he worked extensively with \\nSQL databases, creating generic SQL scripts for data correction requests and building a UI-\\nbased tool for monitoring and manually processing failed transactions. His work \\nstreamlined internal processes, reducing the turnaround time for resolving transactional \\nfailures.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \\nInstitute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \\nBachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \\nRanchi (2017-2021).  \\n \\nWork  Proﬁle \\nSwaraj started his career at GEP Worldwide, where he played an instrumental role in \\noptimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \\ntenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \\nSenior Software Engineer.  \\nAs a Product Development Intern (May 2020 - July 2020, Remote), he developed a React', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \\nzero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \\nReact, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \\nbuilding scalable web applications.  \\nHis expertise in UI/UX and API development allows him to optimize and streamline system \\ninteractions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \\nRedis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \\noptimization in NoSQL and deployed Redis caching solutions to improve query \\nperformance and data retrieval speeds.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='performance and data retrieval speeds.  \\nSwaraj has hands-on experience with PyTorch, TensorFlow, and Hugging Face models, \\ntraining BERT, etc. He also built a Dockerized MLﬂow Server (GCP), integrated with a Flask \\nAPI for ML model versioning.  \\nHe has a deep understanding of networking and security, successfully deploying a personal \\nVPN on Raspberry Pi that allowed a user in China to securely browse the internet via a \\nThailand server. His reverse SSH tunneling system further strengthened remote access \\nsecurity.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is his work experience?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory\n",
    "\n",
    "One of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper that provides convenience methods for saving HumanMessages, AIMessages, and then fetching them all.\n",
    "\n",
    "You may want to use this class directly if you are managing memory outside of a chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_user_message('hi')\n",
    "history.add_ai_message('Whats up?')\n",
    "history.add_user_message('How are you')\n",
    "history.add_ai_message('I\\'m quite good. How about you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[HumanMessage(content='hi'), AIMessage(content='Whats up?'), HumanMessage(content='How are you'), AIMessage(content=\"I'm quite good. How about you?\")])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Memory types\n",
    "\n",
    "There are many different types of memory. Each has their own parameters, their own return types, and is useful in different scenarios. \n",
    "- Converstaion Buffer\n",
    "- Converstaion Buffer Window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What variables get returned from memory\n",
    "\n",
    "Before going into the chain, various variables are read from memory. These have specific names which need to align with the variables the chain expects. You can see what these variables are by calling memory.load_memory_variables({}). Note that the empty dictionary that we pass in is just a placeholder for real variables. If the memory type you are using is dependent upon the input variables, you may need to pass some in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you can see that load_memory_variables returns a single key, history. This means that your chain (and likely your prompt) should expect an input named history. You can usually control this variable through parameters on the memory class. For example, if you want the memory variables to be returned in the key chat_history you can do:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converstaion Buffer\n",
    "This memory allows for storing messages and then extracts the messages in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi\\nAI: What's up?\\nHuman: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content=\"What's up?\"),\n",
       "  HumanMessage(content='How are you?'),\n",
       "  AIMessage(content=\"I'm quite good. How about you?\")]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages = True)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Buffer Window\n",
    "- it keeps a list of the interactions of the conversation over time. \n",
    "- it only uses the last K interactions. \n",
    "- it can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
    "\n",
    "An `LLMChain` is a simple chain that adds some functionality around language models.\n",
    "- it consists of a `PromptTemplate` and a `LM` (either an LLM or chat model).\n",
    "- it formats the prompt template using the input key values provided (and also memory key values, if available), \n",
    "- it passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "Note : [Download Fastchat Model Here](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axiom/anaconda3/envs/NLP6/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",  \n",
    "    temperature=0.8  \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Class ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/_modules/langchain/chains/conversational_retrieval/base.html#ConversationalRetrievalChain)\n",
    "\n",
    "- `retriever` : Retriever to use to fetch documents.\n",
    "\n",
    "- `combine_docs_chain` : The chain used to combine any retrieved documents.\n",
    "\n",
    "- `question_generator`: The chain used to generate a new question for the sake of retrieval. This chain will take in the current question (with variable question) and any chat history (with variable chat_history) and will produce a new standalone question to be used later on.\n",
    "\n",
    "- `return_source_documents` : Return the retrieved source documents as part of the final result.\n",
    "\n",
    "- `get_chat_history` : An optional function to get a string of the chat history. If None is provided, will use a default.\n",
    "\n",
    "- `return_generated_question` : Return the generated question as part of the final result.\n",
    "\n",
    "- `response_if_no_docs_found` : If specified, the chain will return a fixed response if no docs are found for the question.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`question_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = CONDENSE_QUESTION_PROMPT,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axiom/anaconda3/envs/NLP6/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human:What is your current country?\n",
      "AI:\n",
      "Human:What is your previous country?\n",
      "AI:\n",
      "Follow Up Input: Comparing both of them\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human:What is your current country?\\nAI:\\nHuman:What is your previous country?\\nAI:',\n",
       " 'question': 'Comparing both of them',\n",
       " 'text': 'How do the two countries compare?'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Comparing both of them'\n",
    "chat_history = \"Human:What is your current country?\\nAI:\\nHuman:What is your previous country?\\nAI:\"\n",
    "\n",
    "question_generator({'chat_history' : chat_history, \"question\" : query})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combine_docs_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are a helpful assistant that answers questions about the person based on their personal documents.\\n    Use the following context to answer the question. If you don't know the answer, just say you don't know.\\n    Don't make things up.    \\n\\n    Context: {context}\\n    Question: {question}\\n    Answer:\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ba31a527460>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ba31a530dc0>, model_name='gpt-4o-mini', temperature=0.8, openai_api_key='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', openai_proxy='')), document_variable_name='context')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain = load_qa_chain(\n",
    "    llm = llm,\n",
    "    chain_type = 'stuff',\n",
    "    prompt = PROMPT,\n",
    "    verbose = True\n",
    ")\n",
    "doc_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions about the person based on their personal documents.\n",
      "    Use the following context to answer the question. If you don't know the answer, just say you don't know.\n",
      "    Don't make things up.    \n",
      "\n",
      "    Context: As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React \n",
      "Native-based RFID application, created a secure QR code generator with custom \n",
      "encryption, and built a QR code scanner for secure data retrieval. His innovation laid the \n",
      "groundwork for secure mobile-based authentication systems.  \n",
      "In his Technology Internship (Jan 2021 - June 2021, Remote), he worked extensively with \n",
      "SQL databases, creating generic SQL scripts for data correction requests and building a UI-\n",
      "based tool for monitoring and manually processing failed transactions. His work \n",
      "streamlined internal processes, reducing the turnaround time for resolving transactional \n",
      "failures.\n",
      "\n",
      "Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \n",
      "Institute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \n",
      "Bachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \n",
      "Ranchi (2017-2021).  \n",
      " \n",
      "Work  Proﬁle \n",
      "Swaraj started his career at GEP Worldwide, where he played an instrumental role in \n",
      "optimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \n",
      "tenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \n",
      "Senior Software Engineer.  \n",
      "As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React\n",
      "\n",
      "Personal Aspirations \n",
      "Ultimately, Swaraj’s ambition is to become a businessman, leveraging his technical \n",
      "expertise, problem-solving mindset, and strategic vision to create impactful tech solutions.\n",
      "\n",
      "failures. \n",
      "Upon transitioning into a full-time Software Engineer (July 2021 - Dec 2023, Remote), he \n",
      "played a key role in standardizing HTML templates for transactional documents, developing \n",
      "bulk APIs that reduced data correction ticket resolution times from hours to minutes, and \n",
      "enhancing cXML/JSON special character handling to eliminate interface failures. He \n",
      "automated transaction reprocessing, signiﬁcantly reducing manual interventions, and\n",
      "    Question: What is your current job?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React \\nNative-based RFID application, created a secure QR code generator with custom \\nencryption, and built a QR code scanner for secure data retrieval. His innovation laid the \\ngroundwork for secure mobile-based authentication systems.  \\nIn his Technology Internship (Jan 2021 - June 2021, Remote), he worked extensively with \\nSQL databases, creating generic SQL scripts for data correction requests and building a UI-\\nbased tool for monitoring and manually processing failed transactions. His work \\nstreamlined internal processes, reducing the turnaround time for resolving transactional \\nfailures.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \\nInstitute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \\nBachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \\nRanchi (2017-2021).  \\n \\nWork  Proﬁle \\nSwaraj started his career at GEP Worldwide, where he played an instrumental role in \\noptimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \\ntenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \\nSenior Software Engineer.  \\nAs a Product Development Intern (May 2020 - July 2020, Remote), he developed a React', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Personal Aspirations \\nUltimately, Swaraj’s ambition is to become a businessman, leveraging his technical \\nexpertise, problem-solving mindset, and strategic vision to create impactful tech solutions.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 3, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='failures. \\nUpon transitioning into a full-time Software Engineer (July 2021 - Dec 2023, Remote), he \\nplayed a key role in standardizing HTML templates for transactional documents, developing \\nbulk APIs that reduced data correction ticket resolution times from hours to minutes, and \\nenhancing cXML/JSON special character handling to eliminate interface failures. He \\nautomated transaction reprocessing, signiﬁcantly reducing manual interventions, and', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})],\n",
       " 'question': 'What is your current job?',\n",
       " 'output_text': \"I don't know.\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is your current job?\"\n",
    "input_document = retriever.get_relevant_documents(query)\n",
    "\n",
    "doc_chain({'input_documents':input_document, 'question':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationBufferWindowMemory(output_key='answer', return_messages=True, memory_key='chat_history', k=3), verbose=True, combine_docs_chain=StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are a helpful assistant that answers questions about the person based on their personal documents.\\n    Use the following context to answer the question. If you don't know the answer, just say you don't know.\\n    Don't make things up.    \\n\\n    Context: {context}\\n    Question: {question}\\n    Answer:\"), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ba31a527460>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ba31a530dc0>, model_name='gpt-4o-mini', temperature=0.8, openai_api_key='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', openai_proxy='')), document_variable_name='context'), question_generator=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ba31a527460>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ba31a530dc0>, model_name='gpt-4o-mini', temperature=0.8, openai_api_key='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', openai_proxy='')), return_source_documents=True, get_chat_history=<function <lambda> at 0x7ba31a4c7b80>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7ba33df09e80>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3, \n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages = True,\n",
    "    output_key = 'answer'\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    get_chat_history=lambda h : h\n",
    ")\n",
    "chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions about the person based on their personal documents.\n",
      "    Use the following context to answer the question. If you don't know the answer, just say you don't know.\n",
      "    Don't make things up.    \n",
      "\n",
      "    Context: Introduction \n",
      "Swaraj Bhanja is a highly skilled Cloud Engineer, AI Enthusiast, and Technologist with \n",
      "expertise spanning cloud computing, DevOps, full-stack development, machine learning, \n",
      "and AI infrastructure optimization. His journey has been deﬁned by deep technical \n",
      "expertise, an insatiable curiosity for problem-solving, and a relentless drive for efficiency.  \n",
      " \n",
      "Personal Information \n",
      "Born on October 21, 1997, in Jamshedpur, Jharkhand, India, he hails from Kochi, Kerala, \n",
      "and is currently based in Bangkok, Thailand, where he works as an AI Engineer at AI Brain \n",
      "Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian\n",
      "\n",
      "Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \n",
      "Institute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \n",
      "Bachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \n",
      "Ranchi (2017-2021).  \n",
      " \n",
      "Work  Proﬁle \n",
      "Swaraj started his career at GEP Worldwide, where he played an instrumental role in \n",
      "optimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \n",
      "tenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \n",
      "Senior Software Engineer.  \n",
      "As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React\n",
      "\n",
      "As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React \n",
      "Native-based RFID application, created a secure QR code generator with custom \n",
      "encryption, and built a QR code scanner for secure data retrieval. His innovation laid the \n",
      "groundwork for secure mobile-based authentication systems.  \n",
      "In his Technology Internship (Jan 2021 - June 2021, Remote), he worked extensively with \n",
      "SQL databases, creating generic SQL scripts for data correction requests and building a UI-\n",
      "based tool for monitoring and manually processing failed transactions. His work \n",
      "streamlined internal processes, reducing the turnaround time for resolving transactional \n",
      "failures.\n",
      "\n",
      "Personal Aspirations \n",
      "Ultimately, Swaraj’s ambition is to become a businessman, leveraging his technical \n",
      "expertise, problem-solving mindset, and strategic vision to create impactful tech solutions.\n",
      "    Question: Who are you by the way?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who are you by the way?',\n",
       " 'chat_history': [],\n",
       " 'answer': 'I am Swaraj Bhanja, a Cloud Engineer, AI Enthusiast, and Technologist with expertise in cloud computing, DevOps, full-stack development, machine learning, and AI infrastructure optimization. Currently, I work as an AI Engineer at AI Brain Lab and am pursuing my Master’s in Data Science and Artificial Intelligence from the Asian Institute of Technology in Thailand.',\n",
       " 'source_documents': [Document(page_content='Introduction \\nSwaraj Bhanja is a highly skilled Cloud Engineer, AI Enthusiast, and Technologist with \\nexpertise spanning cloud computing, DevOps, full-stack development, machine learning, \\nand AI infrastructure optimization. His journey has been deﬁned by deep technical \\nexpertise, an insatiable curiosity for problem-solving, and a relentless drive for efficiency.  \\n \\nPersonal Information \\nBorn on October 21, 1997, in Jamshedpur, Jharkhand, India, he hails from Kochi, Kerala, \\nand is currently based in Bangkok, Thailand, where he works as an AI Engineer at AI Brain \\nLab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \\nInstitute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \\nBachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \\nRanchi (2017-2021).  \\n \\nWork  Proﬁle \\nSwaraj started his career at GEP Worldwide, where he played an instrumental role in \\noptimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \\ntenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \\nSenior Software Engineer.  \\nAs a Product Development Intern (May 2020 - July 2020, Remote), he developed a React', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React \\nNative-based RFID application, created a secure QR code generator with custom \\nencryption, and built a QR code scanner for secure data retrieval. His innovation laid the \\ngroundwork for secure mobile-based authentication systems.  \\nIn his Technology Internship (Jan 2021 - June 2021, Remote), he worked extensively with \\nSQL databases, creating generic SQL scripts for data correction requests and building a UI-\\nbased tool for monitoring and manually processing failed transactions. His work \\nstreamlined internal processes, reducing the turnaround time for resolving transactional \\nfailures.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Personal Aspirations \\nUltimately, Swaraj’s ambition is to become a businessman, leveraging his technical \\nexpertise, problem-solving mindset, and strategic vision to create impactful tech solutions.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 3, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"Who are you by the way?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='Who are you by the way?'), AIMessage(content='I am Swaraj Bhanja, a Cloud Engineer, AI Enthusiast, and Technologist with expertise in cloud computing, DevOps, full-stack development, machine learning, and AI infrastructure optimization. Currently, I work as an AI Engineer at AI Brain Lab and am pursuing my Master’s in Data Science and Artificial Intelligence from the Asian Institute of Technology in Thailand.')]\n",
      "Follow Up Input: What is your technology stack?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions about the person based on their personal documents.\n",
      "    Use the following context to answer the question. If you don't know the answer, just say you don't know.\n",
      "    Don't make things up.    \n",
      "\n",
      "    Context: CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \n",
      "zero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \n",
      "React, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \n",
      "building scalable web applications.  \n",
      "His expertise in UI/UX and API development allows him to optimize and streamline system \n",
      "interactions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \n",
      "Redis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \n",
      "optimization in NoSQL and deployed Redis caching solutions to improve query \n",
      "performance and data retrieval speeds.\n",
      "\n",
      "As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React \n",
      "Native-based RFID application, created a secure QR code generator with custom \n",
      "encryption, and built a QR code scanner for secure data retrieval. His innovation laid the \n",
      "groundwork for secure mobile-based authentication systems.  \n",
      "In his Technology Internship (Jan 2021 - June 2021, Remote), he worked extensively with \n",
      "SQL databases, creating generic SQL scripts for data correction requests and building a UI-\n",
      "based tool for monitoring and manually processing failed transactions. His work \n",
      "streamlined internal processes, reducing the turnaround time for resolving transactional \n",
      "failures.\n",
      "\n",
      "performance and data retrieval speeds.  \n",
      "Swaraj has hands-on experience with PyTorch, TensorFlow, and Hugging Face models, \n",
      "training BERT, etc. He also built a Dockerized MLﬂow Server (GCP), integrated with a Flask \n",
      "API for ML model versioning.  \n",
      "He has a deep understanding of networking and security, successfully deploying a personal \n",
      "VPN on Raspberry Pi that allowed a user in China to securely browse the internet via a \n",
      "Thailand server. His reverse SSH tunneling system further strengthened remote access \n",
      "security.\n",
      "\n",
      "Notable Projects & Contributions \n",
      "1. Multi-Cloud Web Application: Hosted across AWS, Azure, and DigitalOcean, \n",
      "featuring Kubernetes-based load balancing and zero downtime deployment. \n",
      "2. Web Version of Ubuntu 18.04 (Hackathon Project, 2018): Created a web-based \n",
      "Ubuntu interface as part of a team of 4. \n",
      "3. Custom VPN on Raspberry Pi: Set up a secure VPN that allowed international \n",
      "access to a home network. \n",
      "4. Automated Data Processing Pipelines: Developed mass data correction and auto-\n",
      "reprocessing tools, reducing operational overhead. \n",
      " \n",
      "Personal Interests & Philosophy \n",
      "Swaraj views experience through the lens of dynamic programming, where past\n",
      "    Question: What is your technology stack?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is your technology stack?',\n",
       " 'chat_history': [HumanMessage(content='Who are you by the way?'),\n",
       "  AIMessage(content='I am Swaraj Bhanja, a Cloud Engineer, AI Enthusiast, and Technologist with expertise in cloud computing, DevOps, full-stack development, machine learning, and AI infrastructure optimization. Currently, I work as an AI Engineer at AI Brain Lab and am pursuing my Master’s in Data Science and Artificial Intelligence from the Asian Institute of Technology in Thailand.')],\n",
       " 'answer': 'The technology stack includes React, Angular, Nuxt.js for the frontend, and Django and FastAPI for the backend. It also involves SQL databases like PostgreSQL, MySQL, and SQL Server, as well as NoSQL databases such as MongoDB and Redis, and Graph technologies like GraphQL and Neo4j. Additionally, there is experience with CI/CD automation, cloud platforms (AWS, Azure, GCP, DigitalOcean), and machine learning frameworks like PyTorch and TensorFlow.',\n",
       " 'source_documents': [Document(page_content='CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \\nzero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \\nReact, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \\nbuilding scalable web applications.  \\nHis expertise in UI/UX and API development allows him to optimize and streamline system \\ninteractions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \\nRedis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \\noptimization in NoSQL and deployed Redis caching solutions to improve query \\nperformance and data retrieval speeds.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React \\nNative-based RFID application, created a secure QR code generator with custom \\nencryption, and built a QR code scanner for secure data retrieval. His innovation laid the \\ngroundwork for secure mobile-based authentication systems.  \\nIn his Technology Internship (Jan 2021 - June 2021, Remote), he worked extensively with \\nSQL databases, creating generic SQL scripts for data correction requests and building a UI-\\nbased tool for monitoring and manually processing failed transactions. His work \\nstreamlined internal processes, reducing the turnaround time for resolving transactional \\nfailures.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='performance and data retrieval speeds.  \\nSwaraj has hands-on experience with PyTorch, TensorFlow, and Hugging Face models, \\ntraining BERT, etc. He also built a Dockerized MLﬂow Server (GCP), integrated with a Flask \\nAPI for ML model versioning.  \\nHe has a deep understanding of networking and security, successfully deploying a personal \\nVPN on Raspberry Pi that allowed a user in China to securely browse the internet via a \\nThailand server. His reverse SSH tunneling system further strengthened remote access \\nsecurity.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Notable Projects & Contributions \\n1. Multi-Cloud Web Application: Hosted across AWS, Azure, and DigitalOcean, \\nfeaturing Kubernetes-based load balancing and zero downtime deployment. \\n2. Web Version of Ubuntu 18.04 (Hackathon Project, 2018): Created a web-based \\nUbuntu interface as part of a team of 4. \\n3. Custom VPN on Raspberry Pi: Set up a secure VPN that allowed international \\naccess to a home network. \\n4. Automated Data Processing Pipelines: Developed mass data correction and auto-\\nreprocessing tools, reducing operational overhead. \\n \\nPersonal Interests & Philosophy \\nSwaraj views experience through the lens of dynamic programming, where past', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"What is your technology stack?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='Who are you by the way?'), AIMessage(content='I am Swaraj Bhanja, a Cloud Engineer, AI Enthusiast, and Technologist with expertise in cloud computing, DevOps, full-stack development, machine learning, and AI infrastructure optimization. Currently, I work as an AI Engineer at AI Brain Lab and am pursuing my Master’s in Data Science and Artificial Intelligence from the Asian Institute of Technology in Thailand.'), HumanMessage(content='What is your technology stack?'), AIMessage(content='The technology stack includes React, Angular, Nuxt.js for the frontend, and Django and FastAPI for the backend. It also involves SQL databases like PostgreSQL, MySQL, and SQL Server, as well as NoSQL databases such as MongoDB and Redis, and Graph technologies like GraphQL and Neo4j. Additionally, there is experience with CI/CD automation, cloud platforms (AWS, Azure, GCP, DigitalOcean), and machine learning frameworks like PyTorch and TensorFlow.')]\n",
      "Follow Up Input: Are you familiar with Cloud Computing?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions about the person based on their personal documents.\n",
      "    Use the following context to answer the question. If you don't know the answer, just say you don't know.\n",
      "    Don't make things up.    \n",
      "\n",
      "    Context: CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \n",
      "zero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \n",
      "React, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \n",
      "building scalable web applications.  \n",
      "His expertise in UI/UX and API development allows him to optimize and streamline system \n",
      "interactions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \n",
      "Redis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \n",
      "optimization in NoSQL and deployed Redis caching solutions to improve query \n",
      "performance and data retrieval speeds.\n",
      "\n",
      "Notable Projects & Contributions \n",
      "1. Multi-Cloud Web Application: Hosted across AWS, Azure, and DigitalOcean, \n",
      "featuring Kubernetes-based load balancing and zero downtime deployment. \n",
      "2. Web Version of Ubuntu 18.04 (Hackathon Project, 2018): Created a web-based \n",
      "Ubuntu interface as part of a team of 4. \n",
      "3. Custom VPN on Raspberry Pi: Set up a secure VPN that allowed international \n",
      "access to a home network. \n",
      "4. Automated Data Processing Pipelines: Developed mass data correction and auto-\n",
      "reprocessing tools, reducing operational overhead. \n",
      " \n",
      "Personal Interests & Philosophy \n",
      "Swaraj views experience through the lens of dynamic programming, where past\n",
      "\n",
      "performance and data retrieval speeds.  \n",
      "Swaraj has hands-on experience with PyTorch, TensorFlow, and Hugging Face models, \n",
      "training BERT, etc. He also built a Dockerized MLﬂow Server (GCP), integrated with a Flask \n",
      "API for ML model versioning.  \n",
      "He has a deep understanding of networking and security, successfully deploying a personal \n",
      "VPN on Raspberry Pi that allowed a user in China to securely browse the internet via a \n",
      "Thailand server. His reverse SSH tunneling system further strengthened remote access \n",
      "security.\n",
      "\n",
      "Swaraj is captivated by Christopher Nolan’s ﬁlms, with Interstellar, Inception, Tenet, and \n",
      "Oppenheimer among his favorites. He enjoys technical literature and research on AI and \n",
      "optimization techniques. \n",
      " \n",
      "Future Aspirations \n",
      "Swaraj aims to advance further into Cloud with AI, optimizing multi-cloud architectures for \n",
      "AI-driven applications. His long-term focus is on combining AI, cloud computing, and \n",
      "automation to revolutionize enterprise infrastructure.\n",
      "    Question: Are you familiar with cloud computing?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Are you familiar with Cloud Computing?',\n",
       " 'chat_history': [HumanMessage(content='Who are you by the way?'),\n",
       "  AIMessage(content='I am Swaraj Bhanja, a Cloud Engineer, AI Enthusiast, and Technologist with expertise in cloud computing, DevOps, full-stack development, machine learning, and AI infrastructure optimization. Currently, I work as an AI Engineer at AI Brain Lab and am pursuing my Master’s in Data Science and Artificial Intelligence from the Asian Institute of Technology in Thailand.'),\n",
       "  HumanMessage(content='What is your technology stack?'),\n",
       "  AIMessage(content='The technology stack includes React, Angular, Nuxt.js for the frontend, and Django and FastAPI for the backend. It also involves SQL databases like PostgreSQL, MySQL, and SQL Server, as well as NoSQL databases such as MongoDB and Redis, and Graph technologies like GraphQL and Neo4j. Additionally, there is experience with CI/CD automation, cloud platforms (AWS, Azure, GCP, DigitalOcean), and machine learning frameworks like PyTorch and TensorFlow.')],\n",
       " 'answer': 'Yes, he is familiar with cloud computing, having worked with AWS, Azure, GCP, and DigitalOcean.',\n",
       " 'source_documents': [Document(page_content='CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \\nzero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \\nReact, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \\nbuilding scalable web applications.  \\nHis expertise in UI/UX and API development allows him to optimize and streamline system \\ninteractions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \\nRedis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \\noptimization in NoSQL and deployed Redis caching solutions to improve query \\nperformance and data retrieval speeds.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Notable Projects & Contributions \\n1. Multi-Cloud Web Application: Hosted across AWS, Azure, and DigitalOcean, \\nfeaturing Kubernetes-based load balancing and zero downtime deployment. \\n2. Web Version of Ubuntu 18.04 (Hackathon Project, 2018): Created a web-based \\nUbuntu interface as part of a team of 4. \\n3. Custom VPN on Raspberry Pi: Set up a secure VPN that allowed international \\naccess to a home network. \\n4. Automated Data Processing Pipelines: Developed mass data correction and auto-\\nreprocessing tools, reducing operational overhead. \\n \\nPersonal Interests & Philosophy \\nSwaraj views experience through the lens of dynamic programming, where past', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='performance and data retrieval speeds.  \\nSwaraj has hands-on experience with PyTorch, TensorFlow, and Hugging Face models, \\ntraining BERT, etc. He also built a Dockerized MLﬂow Server (GCP), integrated with a Flask \\nAPI for ML model versioning.  \\nHe has a deep understanding of networking and security, successfully deploying a personal \\nVPN on Raspberry Pi that allowed a user in China to securely browse the internet via a \\nThailand server. His reverse SSH tunneling system further strengthened remote access \\nsecurity.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Swaraj is captivated by Christopher Nolan’s ﬁlms, with Interstellar, Inception, Tenet, and \\nOppenheimer among his favorites. He enjoys technical literature and research on AI and \\noptimization techniques. \\n \\nFuture Aspirations \\nSwaraj aims to advance further into Cloud with AI, optimizing multi-cloud architectures for \\nAI-driven applications. His long-term focus is on combining AI, cloud computing, and \\nautomation to revolutionize enterprise infrastructure.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"Are you familiar with Cloud Computing?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
