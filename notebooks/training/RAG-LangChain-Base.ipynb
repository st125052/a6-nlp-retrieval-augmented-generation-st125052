{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Retrieval-Augmented generation (RAG)\n",
    "\n",
    "RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs.\n",
    "\n",
    "<img src=\"../figures/RAG-process.png\" >\n",
    "\n",
    "Introducing `SwaraBot`, an innovative chatbot that answers questions about Swaraj based on his personal document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompt\n",
    "\n",
    "A set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template=\"You are a helpful assistant that answers questions about the person based on their personal documents.\\n    Use the following context to answer the question. If you don't know the answer, just say you don't know.\\n    Don't make things up.    \\n\\n    Context: {context}\\n    Question: {question}\\n    Answer:\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    You are a helpful assistant that answers questions about the person based on their personal documents.\n",
    "    Use the following context to answer the question. If you don't know the answer, just say you don't know.\n",
    "    Don't make things up.    \n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "    template = prompt_template\n",
    ")\n",
    "\n",
    "PROMPT\n",
    "#using str.format \n",
    "#The placeholder is defined using curly brackets: {} {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a helpful assistant that answers questions about the person based on their personal documents.\\n    Use the following context to answer the question. If you don't know the answer, just say you don't know.\\n    Don't make things up.    \\n\\n    Context: This is a sample context about the person. The person has a background in computer science and has worked on various projects related to artificial intelligence.\\n    Question: What is the person's background?\\n    Answer:\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT.format(\n",
    "    context = \"This is a sample context about the person. The person has a background in computer science and has worked on various projects related to artificial intelligence.\",\n",
    "    question = \"What is the person's background?\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval\n",
    "\n",
    "1. `Document loaders` : Load documents from many different sources (HTML, PDF, code). \n",
    "2. `Document transformers` : One of the essential steps in document retrieval is breaking down a large document into smaller, relevant chunks to enhance the retrieval process.\n",
    "3. `Text embedding models` : Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of text that are similar.\n",
    "4. `Vector stores`: there has emerged a need for databases to support efficient storage and searching of these embeddings.\n",
    "5. `Retrievers` : Once the data is in the database, you still need to retrieve it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Document Loaders \n",
    "Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "nlp_docs = '../pdfs/Swaraj.pdf'\n",
    "\n",
    "loader = PyMuPDFLoader(nlp_docs)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='contributed to building mass update tools that led to a notable decrease in ticket volumes. \\nHis impact extended to bug ﬁxes, outbound interface conﬁgurations, and transaction \\nmonitoring, making him an indispensable asset.  \\nElevated to Senior Software Engineer (Jan 2024 - July 2024, Remote), he expanded his \\nscope by implementing timezone-based PDF exports, developing a centralized API retry \\nmechanism, and enhancing auto-reprocessing for supplier-side transactions. His \\nscheduler solution for detecting and removing stuck transactions further improved \\nefficiency.  \\nCurrently, as an AI Engineer at AI Brain Lab (Jan 2025 - Present, On-Site, Bangkok \\nMetropolitan Region, Thailand), he has successfully dockerized the frontend of the ESG \\napplication, implemented CI/CD pipelines for automated deployment, and optimized \\npackage management service segregation, reducing redundancy in the application. His \\ncontributions have accelerated deployment processes and improved infrastructure \\nefficiency. \\n \\nProfessional Experience \\nSwaraj has extensive expertise in multi-cloud architecture, container orchestration, and \\nCI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \\nzero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \\nReact, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \\nbuilding scalable web applications.  \\nHis expertise in UI/UX and API development allows him to optimize and streamline system \\ninteractions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \\nRedis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \\noptimization in NoSQL and deployed Redis caching solutions to improve query \\nperformance and data retrieval speeds.  \\nSwaraj has hands-on experience with PyTorch, TensorFlow, and Hugging Face models, \\ntraining BERT, etc. He also built a Dockerized MLﬂow Server (GCP), integrated with a Flask \\nAPI for ML model versioning.  \\nHe has a deep understanding of networking and security, successfully deploying a personal \\nVPN on Raspberry Pi that allowed a user in China to securely browse the internet via a \\nThailand server. His reverse SSH tunneling system further strengthened remote access \\nsecurity. \\n \\n', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Document Transformers\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "doc = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \\nInstitute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \\nBachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \\nRanchi (2017-2021).  \\n \\nWork  Proﬁle \\nSwaraj started his career at GEP Worldwide, where he played an instrumental role in \\noptimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \\ntenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \\nSenior Software Engineer.  \\nAs a Product Development Intern (May 2020 - July 2020, Remote), he developed a React', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Text Embedding Models\n",
    "Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axiom/anaconda3/envs/NLP6/lib/python3.9/site-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "model_name = 'hkunlp/instructor-base'\n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = {\"device\" : device}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Vector Stores\n",
    "\n",
    "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate vectorstore\n",
    "vector_path = '../models/vector-store-base'\n",
    "if not os.path.exists(vector_path):\n",
    "    os.makedirs(vector_path)\n",
    "    print('create path done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save vector locally\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents = doc,\n",
    "    embedding = embedding_model\n",
    ")\n",
    "\n",
    "db_file_name = 'nlp_stanford'\n",
    "\n",
    "vectordb.save_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    index_name = 'nlp' #default index\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 retrievers\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling vector from local\n",
    "vector_path = '..models/vector-store-base'\n",
    "db_file_name = 'nlp_stanford'\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.load_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    embeddings = embedding_model,\n",
    "    index_name = 'nlp' #default index\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ready to use\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Personal Aspirations \\nUltimately, Swaraj’s ambition is to become a businessman, leveraging his technical \\nexpertise, problem-solving mindset, and strategic vision to create impactful tech solutions.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 3, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \\nInstitute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \\nBachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \\nRanchi (2017-2021).  \\n \\nWork  Proﬁle \\nSwaraj started his career at GEP Worldwide, where he played an instrumental role in \\noptimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \\ntenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \\nSenior Software Engineer.  \\nAs a Product Development Intern (May 2020 - July 2020, Remote), he developed a React', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Introduction \\nSwaraj Bhanja is a highly skilled Cloud Engineer, AI Enthusiast, and Technologist with \\nexpertise spanning cloud computing, DevOps, full-stack development, machine learning, \\nand AI infrastructure optimization. His journey has been deﬁned by deep technical \\nexpertise, an insatiable curiosity for problem-solving, and a relentless drive for efficiency.  \\n \\nPersonal Information \\nBorn on October 21, 1997, in Jamshedpur, Jharkhand, India, he hails from Kochi, Kerala, \\nand is currently based in Bangkok, Thailand, where he works as an AI Engineer at AI Brain \\nLab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React \\nNative-based RFID application, created a secure QR code generator with custom \\nencryption, and built a QR code scanner for secure data retrieval. His innovation laid the \\ngroundwork for secure mobile-based authentication systems.  \\nIn his Technology Internship (Jan 2021 - June 2021, Remote), he worked extensively with \\nSQL databases, creating generic SQL scripts for data correction requests and building a UI-\\nbased tool for monitoring and manually processing failed transactions. His work \\nstreamlined internal processes, reducing the turnaround time for resolving transactional \\nfailures.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is the person's background?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \\nzero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \\nReact, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \\nbuilding scalable web applications.  \\nHis expertise in UI/UX and API development allows him to optimize and streamline system \\ninteractions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \\nRedis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \\noptimization in NoSQL and deployed Redis caching solutions to improve query \\nperformance and data retrieval speeds.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \\nInstitute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \\nBachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \\nRanchi (2017-2021).  \\n \\nWork  Proﬁle \\nSwaraj started his career at GEP Worldwide, where he played an instrumental role in \\noptimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \\ntenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \\nSenior Software Engineer.  \\nAs a Product Development Intern (May 2020 - July 2020, Remote), he developed a React', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React \\nNative-based RFID application, created a secure QR code generator with custom \\nencryption, and built a QR code scanner for secure data retrieval. His innovation laid the \\ngroundwork for secure mobile-based authentication systems.  \\nIn his Technology Internship (Jan 2021 - June 2021, Remote), he worked extensively with \\nSQL databases, creating generic SQL scripts for data correction requests and building a UI-\\nbased tool for monitoring and manually processing failed transactions. His work \\nstreamlined internal processes, reducing the turnaround time for resolving transactional \\nfailures.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       " Document(page_content='failures. \\nUpon transitioning into a full-time Software Engineer (July 2021 - Dec 2023, Remote), he \\nplayed a key role in standardizing HTML templates for transactional documents, developing \\nbulk APIs that reduced data correction ticket resolution times from hours to minutes, and \\nenhancing cXML/JSON special character handling to eliminate interface failures. He \\nautomated transaction reprocessing, signiﬁcantly reducing manual interventions, and', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is his work experience?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory\n",
    "\n",
    "One of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper that provides convenience methods for saving HumanMessages, AIMessages, and then fetching them all.\n",
    "\n",
    "You may want to use this class directly if you are managing memory outside of a chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_user_message('hi')\n",
    "history.add_ai_message('Whats up?')\n",
    "history.add_user_message('How are you')\n",
    "history.add_ai_message('I\\'m quite good. How about you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[HumanMessage(content='hi'), AIMessage(content='Whats up?'), HumanMessage(content='How are you'), AIMessage(content=\"I'm quite good. How about you?\")])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Memory types\n",
    "\n",
    "There are many different types of memory. Each has their own parameters, their own return types, and is useful in different scenarios. \n",
    "- Converstaion Buffer\n",
    "- Converstaion Buffer Window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What variables get returned from memory\n",
    "\n",
    "Before going into the chain, various variables are read from memory. These have specific names which need to align with the variables the chain expects. You can see what these variables are by calling memory.load_memory_variables({}). Note that the empty dictionary that we pass in is just a placeholder for real variables. If the memory type you are using is dependent upon the input variables, you may need to pass some in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you can see that load_memory_variables returns a single key, history. This means that your chain (and likely your prompt) should expect an input named history. You can usually control this variable through parameters on the memory class. For example, if you want the memory variables to be returned in the key chat_history you can do:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converstaion Buffer\n",
    "This memory allows for storing messages and then extracts the messages in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi\\nAI: What's up?\\nHuman: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content=\"What's up?\"),\n",
       "  HumanMessage(content='How are you?'),\n",
       "  AIMessage(content=\"I'm quite good. How about you?\")]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages = True)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Buffer Window\n",
    "- it keeps a list of the interactions of the conversation over time. \n",
    "- it only uses the last K interactions. \n",
    "- it can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
    "\n",
    "An `LLMChain` is a simple chain that adds some functionality around language models.\n",
    "- it consists of a `PromptTemplate` and a `LM` (either an LLM or chat model).\n",
    "- it formats the prompt template using the input key values provided (and also memory key values, if available), \n",
    "- it passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "Note : [Download Fastchat Model Here](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fastchat-t5-3b-v1.0'...\n",
      "remote: Enumerating objects: 45, done.\u001b[K\n",
      "remote: Total 45 (delta 0), reused 0 (delta 0), pack-reused 45 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (45/45), 8.84 KiB | 292.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "# %cd ../helper\n",
    "# !git clone https://huggingface.co/lmsys/fastchat-t5-3b-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "model_id = '../helper/fastchat-t5-3b-v1.0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "bitsandbyte_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bitsandbyte_config, #caution Nvidia\n",
    "    device_map = 'auto',\n",
    "    load_in_8bit = True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens = 256,\n",
    "    model_kwargs = {\n",
    "        \"temperature\" : 0,\n",
    "        \"repetition_penalty\": 1.5\n",
    "    }\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Class ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/_modules/langchain/chains/conversational_retrieval/base.html#ConversationalRetrievalChain)\n",
    "\n",
    "- `retriever` : Retriever to use to fetch documents.\n",
    "\n",
    "- `combine_docs_chain` : The chain used to combine any retrieved documents.\n",
    "\n",
    "- `question_generator`: The chain used to generate a new question for the sake of retrieval. This chain will take in the current question (with variable question) and any chat history (with variable chat_history) and will produce a new standalone question to be used later on.\n",
    "\n",
    "- `return_source_documents` : Return the retrieved source documents as part of the final result.\n",
    "\n",
    "- `get_chat_history` : An optional function to get a string of the chat history. If None is provided, will use a default.\n",
    "\n",
    "- `return_generated_question` : Return the generated question as part of the final result.\n",
    "\n",
    "- `response_if_no_docs_found` : If specified, the chain will return a fixed response if no docs are found for the question.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`question_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = CONDENSE_QUESTION_PROMPT,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axiom/anaconda3/envs/NLP6/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human:What is your current country?\n",
      "AI:\n",
      "Human:What is your previous country?\n",
      "AI:\n",
      "Follow Up Input: Comparing both of them\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human:What is your current country?\\nAI:\\nHuman:What is your previous country?\\nAI:',\n",
       " 'question': 'Comparing both of them',\n",
       " 'text': '<pad> What  is  the  difference  between  your  current  and  previous  country?\\n'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Comparing both of them'\n",
    "chat_history = \"Human:What is your current country?\\nAI:\\nHuman:What is your previous country?\\nAI:\"\n",
    "\n",
    "question_generator({'chat_history' : chat_history, \"question\" : query})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combine_docs_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are a helpful assistant that answers questions about the person based on their personal documents.\\n    Use the following context to answer the question. If you don't know the answer, just say you don't know.\\n    Don't make things up.    \\n\\n    Context: {context}\\n    Question: {question}\\n    Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x779950ead490>)), document_variable_name='context')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain = load_qa_chain(\n",
    "    llm = llm,\n",
    "    chain_type = 'stuff',\n",
    "    prompt = PROMPT,\n",
    "    verbose = True\n",
    ")\n",
    "doc_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions about the person based on their personal documents.\n",
      "    Use the following context to answer the question. If you don't know the answer, just say you don't know.\n",
      "    Don't make things up.    \n",
      "\n",
      "    Context: failures. \n",
      "Upon transitioning into a full-time Software Engineer (July 2021 - Dec 2023, Remote), he \n",
      "played a key role in standardizing HTML templates for transactional documents, developing \n",
      "bulk APIs that reduced data correction ticket resolution times from hours to minutes, and \n",
      "enhancing cXML/JSON special character handling to eliminate interface failures. He \n",
      "automated transaction reprocessing, signiﬁcantly reducing manual interventions, and\n",
      "\n",
      "Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \n",
      "Institute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \n",
      "Bachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \n",
      "Ranchi (2017-2021).  \n",
      " \n",
      "Work  Proﬁle \n",
      "Swaraj started his career at GEP Worldwide, where he played an instrumental role in \n",
      "optimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \n",
      "tenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \n",
      "Senior Software Engineer.  \n",
      "As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React\n",
      "\n",
      "Personal Aspirations \n",
      "Ultimately, Swaraj’s ambition is to become a businessman, leveraging his technical \n",
      "expertise, problem-solving mindset, and strategic vision to create impactful tech solutions.\n",
      "\n",
      "CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \n",
      "zero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \n",
      "React, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \n",
      "building scalable web applications.  \n",
      "His expertise in UI/UX and API development allows him to optimize and streamline system \n",
      "interactions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \n",
      "Redis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \n",
      "optimization in NoSQL and deployed Redis caching solutions to improve query \n",
      "performance and data retrieval speeds.\n",
      "    Question: What is your current job?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='failures. \\nUpon transitioning into a full-time Software Engineer (July 2021 - Dec 2023, Remote), he \\nplayed a key role in standardizing HTML templates for transactional documents, developing \\nbulk APIs that reduced data correction ticket resolution times from hours to minutes, and \\nenhancing cXML/JSON special character handling to eliminate interface failures. He \\nautomated transaction reprocessing, signiﬁcantly reducing manual interventions, and', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \\nInstitute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \\nBachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \\nRanchi (2017-2021).  \\n \\nWork  Proﬁle \\nSwaraj started his career at GEP Worldwide, where he played an instrumental role in \\noptimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \\ntenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \\nSenior Software Engineer.  \\nAs a Product Development Intern (May 2020 - July 2020, Remote), he developed a React', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Personal Aspirations \\nUltimately, Swaraj’s ambition is to become a businessman, leveraging his technical \\nexpertise, problem-solving mindset, and strategic vision to create impactful tech solutions.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 3, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \\nzero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \\nReact, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \\nbuilding scalable web applications.  \\nHis expertise in UI/UX and API development allows him to optimize and streamline system \\ninteractions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \\nRedis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \\noptimization in NoSQL and deployed Redis caching solutions to improve query \\nperformance and data retrieval speeds.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})],\n",
       " 'question': 'What is your current job?',\n",
       " 'output_text': \"<pad> I  don't  know.\\n\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is your current job?\"\n",
    "input_document = retriever.get_relevant_documents(query)\n",
    "\n",
    "doc_chain({'input_documents':input_document, 'question':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationBufferWindowMemory(output_key='answer', return_messages=True, memory_key='chat_history', k=3), verbose=True, combine_docs_chain=StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are a helpful assistant that answers questions about the person based on their personal documents.\\n    Use the following context to answer the question. If you don't know the answer, just say you don't know.\\n    Don't make things up.    \\n\\n    Context: {context}\\n    Question: {question}\\n    Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x779950ead490>)), document_variable_name='context'), question_generator=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x779950ead490>)), return_source_documents=True, get_chat_history=<function <lambda> at 0x7798bb778ca0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7797efa33dc0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3, \n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages = True,\n",
    "    output_key = 'answer'\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    get_chat_history=lambda h : h\n",
    ")\n",
    "chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions about the person based on their personal documents.\n",
      "    Use the following context to answer the question. If you don't know the answer, just say you don't know.\n",
      "    Don't make things up.    \n",
      "\n",
      "    Context: Introduction \n",
      "Swaraj Bhanja is a highly skilled Cloud Engineer, AI Enthusiast, and Technologist with \n",
      "expertise spanning cloud computing, DevOps, full-stack development, machine learning, \n",
      "and AI infrastructure optimization. His journey has been deﬁned by deep technical \n",
      "expertise, an insatiable curiosity for problem-solving, and a relentless drive for efficiency.  \n",
      " \n",
      "Personal Information \n",
      "Born on October 21, 1997, in Jamshedpur, Jharkhand, India, he hails from Kochi, Kerala, \n",
      "and is currently based in Bangkok, Thailand, where he works as an AI Engineer at AI Brain \n",
      "Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian\n",
      "\n",
      "Personal Aspirations \n",
      "Ultimately, Swaraj’s ambition is to become a businessman, leveraging his technical \n",
      "expertise, problem-solving mindset, and strategic vision to create impactful tech solutions.\n",
      "\n",
      "Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \n",
      "Institute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \n",
      "Bachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \n",
      "Ranchi (2017-2021).  \n",
      " \n",
      "Work  Proﬁle \n",
      "Swaraj started his career at GEP Worldwide, where he played an instrumental role in \n",
      "optimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \n",
      "tenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \n",
      "Senior Software Engineer.  \n",
      "As a Product Development Intern (May 2020 - July 2020, Remote), he developed a React\n",
      "\n",
      "Swaraj views experience through the lens of dynamic programming, where past \n",
      "optimizations lead to better decision-making. His passion lies in building efficient cloud \n",
      "systems, where automation reduces redundancy. \n",
      " \n",
      "Music Preferences \n",
      "He is a fan of Metallica and Guns N’ Roses, considering \"Enter Sandman\" one of Metallica’s \n",
      "greatest tracks and \"Sweet Child O’ Mine\" the most iconic song by Guns N' Roses. He \n",
      "theorizes that musicians enter an autonomous state in live performances, instinctively \n",
      "playing the right beats and notes without consciously thinking. \n",
      " \n",
      "Movies & Books \n",
      "Swaraj is captivated by Christopher Nolan’s ﬁlms, with Interstellar, Inception, Tenet, and\n",
      "    Question: Who are you by the way?\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who are you by the way?',\n",
       " 'chat_history': [],\n",
       " 'answer': '<pad>  I  am  an  AI  assistant  and  do  not  have  personal  information.\\n',\n",
       " 'source_documents': [Document(page_content='Introduction \\nSwaraj Bhanja is a highly skilled Cloud Engineer, AI Enthusiast, and Technologist with \\nexpertise spanning cloud computing, DevOps, full-stack development, machine learning, \\nand AI infrastructure optimization. His journey has been deﬁned by deep technical \\nexpertise, an insatiable curiosity for problem-solving, and a relentless drive for efficiency.  \\n \\nPersonal Information \\nBorn on October 21, 1997, in Jamshedpur, Jharkhand, India, he hails from Kochi, Kerala, \\nand is currently based in Bangkok, Thailand, where he works as an AI Engineer at AI Brain \\nLab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Personal Aspirations \\nUltimately, Swaraj’s ambition is to become a businessman, leveraging his technical \\nexpertise, problem-solving mindset, and strategic vision to create impactful tech solutions.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 3, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Lab while pursuing his Master’s in Data Science and Artiﬁcial Intelligence from the Asian \\nInstitute of Technology (AIT), Thailand (August 2024 – May 2026). He completed his \\nBachelor’s in Computer Science and Engineering from Birla Institute of Technology, Mesra, \\nRanchi (2017-2021).  \\n \\nWork  Proﬁle \\nSwaraj started his career at GEP Worldwide, where he played an instrumental role in \\noptimizing internal tools, automating workﬂows, and solving over 8700 JIRA tickets. His \\ntenure spanned multiple roles over 3.6 years, beginning as an intern and culminating as a \\nSenior Software Engineer.  \\nAs a Product Development Intern (May 2020 - July 2020, Remote), he developed a React', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Swaraj views experience through the lens of dynamic programming, where past \\noptimizations lead to better decision-making. His passion lies in building efficient cloud \\nsystems, where automation reduces redundancy. \\n \\nMusic Preferences \\nHe is a fan of Metallica and Guns N’ Roses, considering \"Enter Sandman\" one of Metallica’s \\ngreatest tracks and \"Sweet Child O’ Mine\" the most iconic song by Guns N\\' Roses. He \\ntheorizes that musicians enter an autonomous state in live performances, instinctively \\nplaying the right beats and notes without consciously thinking. \\n \\nMovies & Books \\nSwaraj is captivated by Christopher Nolan’s ﬁlms, with Interstellar, Inception, Tenet, and', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"Who are you by the way?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='Who are you by the way?'), AIMessage(content='<pad>  I  am  an  AI  assistant  and  do  not  have  personal  information.\\n')]\n",
      "Follow Up Input: What is your technology stack?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions about the person based on their personal documents.\n",
      "    Use the following context to answer the question. If you don't know the answer, just say you don't know.\n",
      "    Don't make things up.    \n",
      "\n",
      "    Context: Notable Projects & Contributions \n",
      "1. Multi-Cloud Web Application: Hosted across AWS, Azure, and DigitalOcean, \n",
      "featuring Kubernetes-based load balancing and zero downtime deployment. \n",
      "2. Web Version of Ubuntu 18.04 (Hackathon Project, 2018): Created a web-based \n",
      "Ubuntu interface as part of a team of 4. \n",
      "3. Custom VPN on Raspberry Pi: Set up a secure VPN that allowed international \n",
      "access to a home network. \n",
      "4. Automated Data Processing Pipelines: Developed mass data correction and auto-\n",
      "reprocessing tools, reducing operational overhead. \n",
      " \n",
      "Personal Interests & Philosophy \n",
      "Swaraj views experience through the lens of dynamic programming, where past\n",
      "\n",
      "failures. \n",
      "Upon transitioning into a full-time Software Engineer (July 2021 - Dec 2023, Remote), he \n",
      "played a key role in standardizing HTML templates for transactional documents, developing \n",
      "bulk APIs that reduced data correction ticket resolution times from hours to minutes, and \n",
      "enhancing cXML/JSON special character handling to eliminate interface failures. He \n",
      "automated transaction reprocessing, signiﬁcantly reducing manual interventions, and\n",
      "\n",
      "CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \n",
      "zero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \n",
      "React, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \n",
      "building scalable web applications.  \n",
      "His expertise in UI/UX and API development allows him to optimize and streamline system \n",
      "interactions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \n",
      "Redis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \n",
      "optimization in NoSQL and deployed Redis caching solutions to improve query \n",
      "performance and data retrieval speeds.\n",
      "\n",
      "performance and data retrieval speeds.  \n",
      "Swaraj has hands-on experience with PyTorch, TensorFlow, and Hugging Face models, \n",
      "training BERT, etc. He also built a Dockerized MLﬂow Server (GCP), integrated with a Flask \n",
      "API for ML model versioning.  \n",
      "He has a deep understanding of networking and security, successfully deploying a personal \n",
      "VPN on Raspberry Pi that allowed a user in China to securely browse the internet via a \n",
      "Thailand server. His reverse SSH tunneling system further strengthened remote access \n",
      "security.\n",
      "    Question: <pad> What  is  your  programming  language  of  choice?\n",
      "\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is your technology stack?',\n",
       " 'chat_history': [HumanMessage(content='Who are you by the way?'),\n",
       "  AIMessage(content='<pad>  I  am  an  AI  assistant  and  do  not  have  personal  information.\\n')],\n",
       " 'answer': '<pad>  React,  Angular,  Nuxt.js  (Frontend)  and  Django,  FastAPI  (Backend).\\n',\n",
       " 'source_documents': [Document(page_content='Notable Projects & Contributions \\n1. Multi-Cloud Web Application: Hosted across AWS, Azure, and DigitalOcean, \\nfeaturing Kubernetes-based load balancing and zero downtime deployment. \\n2. Web Version of Ubuntu 18.04 (Hackathon Project, 2018): Created a web-based \\nUbuntu interface as part of a team of 4. \\n3. Custom VPN on Raspberry Pi: Set up a secure VPN that allowed international \\naccess to a home network. \\n4. Automated Data Processing Pipelines: Developed mass data correction and auto-\\nreprocessing tools, reducing operational overhead. \\n \\nPersonal Interests & Philosophy \\nSwaraj views experience through the lens of dynamic programming, where past', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='failures. \\nUpon transitioning into a full-time Software Engineer (July 2021 - Dec 2023, Remote), he \\nplayed a key role in standardizing HTML templates for transactional documents, developing \\nbulk APIs that reduced data correction ticket resolution times from hours to minutes, and \\nenhancing cXML/JSON special character handling to eliminate interface failures. He \\nautomated transaction reprocessing, signiﬁcantly reducing manual interventions, and', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 0, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \\nzero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \\nReact, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \\nbuilding scalable web applications.  \\nHis expertise in UI/UX and API development allows him to optimize and streamline system \\ninteractions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \\nRedis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \\noptimization in NoSQL and deployed Redis caching solutions to improve query \\nperformance and data retrieval speeds.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='performance and data retrieval speeds.  \\nSwaraj has hands-on experience with PyTorch, TensorFlow, and Hugging Face models, \\ntraining BERT, etc. He also built a Dockerized MLﬂow Server (GCP), integrated with a Flask \\nAPI for ML model versioning.  \\nHe has a deep understanding of networking and security, successfully deploying a personal \\nVPN on Raspberry Pi that allowed a user in China to securely browse the internet via a \\nThailand server. His reverse SSH tunneling system further strengthened remote access \\nsecurity.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"What is your technology stack?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='Who are you by the way?'), AIMessage(content='<pad>  I  am  an  AI  assistant  and  do  not  have  personal  information.\\n'), HumanMessage(content='What is your technology stack?'), AIMessage(content='<pad>  React,  Angular,  Nuxt.js  (Frontend)  and  Django,  FastAPI  (Backend).\\n')]\n",
      "Follow Up Input: Are you familiar with Cloud Computing?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions about the person based on their personal documents.\n",
      "    Use the following context to answer the question. If you don't know the answer, just say you don't know.\n",
      "    Don't make things up.    \n",
      "\n",
      "    Context: Swaraj is captivated by Christopher Nolan’s ﬁlms, with Interstellar, Inception, Tenet, and \n",
      "Oppenheimer among his favorites. He enjoys technical literature and research on AI and \n",
      "optimization techniques. \n",
      " \n",
      "Future Aspirations \n",
      "Swaraj aims to advance further into Cloud with AI, optimizing multi-cloud architectures for \n",
      "AI-driven applications. His long-term focus is on combining AI, cloud computing, and \n",
      "automation to revolutionize enterprise infrastructure.\n",
      "\n",
      "CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \n",
      "zero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \n",
      "React, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \n",
      "building scalable web applications.  \n",
      "His expertise in UI/UX and API development allows him to optimize and streamline system \n",
      "interactions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \n",
      "Redis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \n",
      "optimization in NoSQL and deployed Redis caching solutions to improve query \n",
      "performance and data retrieval speeds.\n",
      "\n",
      "Notable Projects & Contributions \n",
      "1. Multi-Cloud Web Application: Hosted across AWS, Azure, and DigitalOcean, \n",
      "featuring Kubernetes-based load balancing and zero downtime deployment. \n",
      "2. Web Version of Ubuntu 18.04 (Hackathon Project, 2018): Created a web-based \n",
      "Ubuntu interface as part of a team of 4. \n",
      "3. Custom VPN on Raspberry Pi: Set up a secure VPN that allowed international \n",
      "access to a home network. \n",
      "4. Automated Data Processing Pipelines: Developed mass data correction and auto-\n",
      "reprocessing tools, reducing operational overhead. \n",
      " \n",
      "Personal Interests & Philosophy \n",
      "Swaraj views experience through the lens of dynamic programming, where past\n",
      "\n",
      "efficiency.  \n",
      "Currently, as an AI Engineer at AI Brain Lab (Jan 2025 - Present, On-Site, Bangkok \n",
      "Metropolitan Region, Thailand), he has successfully dockerized the frontend of the ESG \n",
      "application, implemented CI/CD pipelines for automated deployment, and optimized \n",
      "package management service segregation, reducing redundancy in the application. His \n",
      "contributions have accelerated deployment processes and improved infrastructure \n",
      "efficiency. \n",
      " \n",
      "Professional Experience \n",
      "Swaraj has extensive expertise in multi-cloud architecture, container orchestration, and \n",
      "CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing\n",
      "    Question: <pad> What  is  Cloud  Computing?\n",
      "\n",
      "    Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Are you familiar with Cloud Computing?',\n",
       " 'chat_history': [HumanMessage(content='Who are you by the way?'),\n",
       "  AIMessage(content='<pad>  I  am  an  AI  assistant  and  do  not  have  personal  information.\\n'),\n",
       "  HumanMessage(content='What is your technology stack?'),\n",
       "  AIMessage(content='<pad>  React,  Angular,  Nuxt.js  (Frontend)  and  Django,  FastAPI  (Backend).\\n')],\n",
       " 'answer': '<pad>  It  is  a  type  of  computing  where  organizations  rent  computing  resources  from  a  third-party  provider  such  as  Amazon  Web  Services  (AWS),  Microsoft  Azure,  Google  Cloud  Platform  (GCP),  or  DigitalOcean.  The  provider  manages  the  infrastructure  and  provides  the  resources  needed  to  run  the  application.  This  allows  organizations  to  focus  on  their  core  business  while  the  provider  handles  the  infrastructure  and  maintenance.\\n',\n",
       " 'source_documents': [Document(page_content='Swaraj is captivated by Christopher Nolan’s ﬁlms, with Interstellar, Inception, Tenet, and \\nOppenheimer among his favorites. He enjoys technical literature and research on AI and \\noptimization techniques. \\n \\nFuture Aspirations \\nSwaraj aims to advance further into Cloud with AI, optimizing multi-cloud architectures for \\nAI-driven applications. His long-term focus is on combining AI, cloud computing, and \\nautomation to revolutionize enterprise infrastructure.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='CI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing \\nzero downtime deployment strategies like Blue-Green Deployment. He is proﬁcient in \\nReact, Angular, Nuxt.js (Frontend) and Django, FastAPI (Backend), with a strong focus on \\nbuilding scalable web applications.  \\nHis expertise in UI/UX and API development allows him to optimize and streamline system \\ninteractions. His knowledge of SQL (PostgreSQL, MySQL, SQL Server), NoSQL (MongoDB, \\nRedis) and Graph (GraphQL, Neo4j) is extensive. He has worked on embedding-based \\noptimization in NoSQL and deployed Redis caching solutions to improve query \\nperformance and data retrieval speeds.', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Notable Projects & Contributions \\n1. Multi-Cloud Web Application: Hosted across AWS, Azure, and DigitalOcean, \\nfeaturing Kubernetes-based load balancing and zero downtime deployment. \\n2. Web Version of Ubuntu 18.04 (Hackathon Project, 2018): Created a web-based \\nUbuntu interface as part of a team of 4. \\n3. Custom VPN on Raspberry Pi: Set up a secure VPN that allowed international \\naccess to a home network. \\n4. Automated Data Processing Pipelines: Developed mass data correction and auto-\\nreprocessing tools, reducing operational overhead. \\n \\nPersonal Interests & Philosophy \\nSwaraj views experience through the lens of dynamic programming, where past', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''}),\n",
       "  Document(page_content='efficiency.  \\nCurrently, as an AI Engineer at AI Brain Lab (Jan 2025 - Present, On-Site, Bangkok \\nMetropolitan Region, Thailand), he has successfully dockerized the frontend of the ESG \\napplication, implemented CI/CD pipelines for automated deployment, and optimized \\npackage management service segregation, reducing redundancy in the application. His \\ncontributions have accelerated deployment processes and improved infrastructure \\nefficiency. \\n \\nProfessional Experience \\nSwaraj has extensive expertise in multi-cloud architecture, container orchestration, and \\nCI/CD automation. He has worked with AWS, Azure, GCP, and DigitalOcean, implementing', metadata={'source': './Swaraj.pdf', 'file_path': './Swaraj.pdf', 'page': 1, 'total_pages': 4, 'format': 'PDF 1.6', 'title': '', 'author': 'Swaraj Bhanja', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.5.197', 'creationDate': \"D:20250309193041+07'00'\", 'modDate': \"D:20250309193043+07'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = \"Are you familiar with Cloud Computing?\"\n",
    "answer = chain({\"question\":prompt_question})\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
